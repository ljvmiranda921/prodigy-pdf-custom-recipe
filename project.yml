title: "Prodigy recipes for document processing and layout understanding"
description: |
  This repository contains recipes on how to use [Prodigy](https://prodi.gy) and
  [Hugging Face](https://huggingface.co) for annotating, training, and reviewing
  document layout datasets.  We'll be finetuning a
  [LayoutLMv3](https://arxiv.org/abs/2204.08387) model using
  [FUNSD](https://guillaumejaume.github.io/FUNSD/), a dataset of noisy scanned
  documents.

  ![](docs/prodigy_annotation.gif)

  This also serves as an illustration of how to design document processing
  solutions. I attempted to generalize this approach into a framework, which you
  can read more [from my
  blog.](https://ljvmiranda921.github.io/notebook/2022/07/02/a-document-processing-framework/)

  ![](docs/design_framework.png)

directories:
  - "assets"
  - "dataset"
  - "model"

vars:
  dataset_name: funsd

assets:
  - dest: "assets/${vars.dataset_name}.zip"
    description: "FUNSD dataset - noisy scanned documents for layout understanding"
    url: https://guillaumejaume.github.io/FUNSD/dataset.zip

workflows:
  all:
    - "install"
    - "hydrate-db"
    - "train"
  clean-all:
    - "clean-db"
    - "clean-files"

commands:
  - name: "install"
    help: "Install dependencies"
    script:
      - "pip install --user -r requirements.txt"
      - "sudo apt install tesseract-ocr -y"

  - name: "hydrate-db"
    help: "Hydrate the Prodigy database with annotated data from FUNSD"
    script:
      - "python3 -m scripts.preprocess assets/${vars.dataset_name}.zip dataset/"
      - "python3 -m prodigy db-in-image ${vars.dataset_name} -F scripts/recipes/loaders.py"
    deps:
      - assets/${vars.dataset_name}.zip

  - name: "review"
    help: "Review hydrated annotations"
    script:
      - "python3 -m prodigy db-out ${vars.dataset_name} dataset"
      - >-
        python3 -m prodigy image.manual 
        ${vars.dataset_name}-debug 
        dataset/${vars.dataset_name}.jsonl 
        --loader jsonl 
        --label question,answer,header,other
    outputs:
      - dataset/funsd.jsonl

  - name: "train"
    help: "Train FUNSD model"
    script:
      - >-
        python3 -m prodigy train-image ${vars.dataset_name} model/
        -F scripts/recipes/train.py --verbose --eval
    outputs:
      - model/checkpoint-500/
      - model/checkpoint-1000/

  - name: "qa"
    help: "Perform QA for the test dataset using a trained model"
    script:
      - >-
        python3 -m prodigy image.qa ${vars.dataset_name}-qa
        dataset/dataset/testing_data/images
        dataset/dataset/testing_data/annotations
        model/checkpoint-1000/
        --label other,header,question,answer
        -F scripts/recipes/qa.py
    deps:
      - model/checkpoint-1000/
      - dataset/dataset/testing_data/images
      - dataset/dataset/testing_data/annotations

  - name: "clean-db"
    help: "Drop all generated Prodigy datasets"
    script:
      - "prodigy drop ${vars.dataset_name}"
      - "prodigy drop ${vars.dataset_name}-debug"

  - name: "clean-files"
    help: "Clean all intermediary files"
    script:
      - "sh -c 'rm -rf assets/*'"
      - "sh -c 'rm -rf dataset/*'"
